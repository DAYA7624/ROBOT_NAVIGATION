{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pretty Table</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t \tMACHINE LEARNING MODELS MODELS\n",
      "+----------------------+------------------------------------+----------+\n",
      "|        Models        |       Best-Hyper-Parameters        | Accuracy |\n",
      "+----------------------+------------------------------------+----------+\n",
      "| Logistic Regression  |         C: 1, penalty: l1          | 65.00 %  |\n",
      "|        ------        |            ------------            |  ------  |\n",
      "|      Linear SVC      |               C: 0.5               | 64.82 %  |\n",
      "|        ------        |            -----------             |  ------  |\n",
      "|  Rbf SVM classifier  |        C: 20, gamma: 0.0078        | 78.48 %  |\n",
      "|        ------        |            -----------             |  ------  |\n",
      "|    DecisionTree      | max_depth: 9, min_samples_split: 5 | 70.60 %  |\n",
      "|        ------        |            ------------            |  ------  |\n",
      "|    Random Forest     | max_depth: 30, n_estimators: 1000  | 83.81 %  |\n",
      "|        ------        |            ------------            |  ------  |\n",
      "| GradientBoosting DT  |  max_depth: 6, n_estimators: 160   | 86.00 %  |\n",
      "|        ------        |            ------------            |  ------  |\n",
      "|      XG-Boost        |  max_depth: 8, n_estimators: 352   | 84.33 %  |\n",
      "+----------------------+------------------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "# pretty table\n",
    "print(\"\")\n",
    "print(\"\\t \\tMACHINE LEARNING MODELS MODELS\")\n",
    "from prettytable import PrettyTable\n",
    "T = PrettyTable()\n",
    "T.field_names = [\"Models\",\"Best-Hyper-Parameters\", \"Accuracy\"]\n",
    "T.add_row([\"Logistic Regression\",\"C: 1, penalty: l1\", \"65.00 %\"])\n",
    "T.add_row([\"------\",\"------------\",\"------\"])\n",
    "T.add_row([\"Linear SVC\",\"C: 0.5\", \"64.82 %\"])\n",
    "T.add_row([\"------\",\"-----------\",\"------\"])\n",
    "T.add_row([\"Rbf SVM classifier\",\"C: 20, gamma: 0.0078\", \"78.48 %\"])\n",
    "T.add_row([\"------\",\"-----------\",\"------\"])\n",
    "T.add_row([\"DecisionTree \",\"max_depth: 9, min_samples_split: 5\", \"70.60 %\"])\n",
    "T.add_row([\"------\",\"------------\",\"------\"])\n",
    "T.add_row([\"Random Forest \",\"max_depth: 30, n_estimators: 1000\", \"83.81 %\"])\n",
    "T.add_row([\"------\",\"------------\",\"------\"])\n",
    "T.add_row([\"GradientBoosting DT \",\"max_depth: 6, n_estimators: 160\", \"86.00 %\"])\n",
    "T.add_row([\"------\",\"------------\",\"------\"])\n",
    "T.add_row([\"XG-Boost \",\"max_depth: 8, n_estimators: 352\", \"84.33 %\"])\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t \tDEEP LEARNING Architectures \n",
      "\n",
      " MODEL-1 Architecture: 1-Layers LSTM\n",
      " LSTM1(100) - Dense(output) ->> Rmsprop+Dropout(0.4)\n",
      "\n",
      " MODEL-2 Architecture: 1-Layer LSTM:\n",
      " LSTM1(40) - Dense(output) ->> Adam+Dropout(0.2)\n",
      "\n",
      " MODEL-3 Architecture: 2-Layer MLP:\n",
      " Dense(500) - Dense(200) - Dense(output) ->> Adam+Dropout(0.3)+BN()\n",
      "\n",
      "+---------+--------------------------+-----------+----------+\n",
      "|  Models |           Data           | Test-Loss | Accuracy |\n",
      "+---------+--------------------------+-----------+----------+\n",
      "| Model-1 |     Raw Signal Data      |   0.9572  | 68.33 %  |\n",
      "|  ------ |       ------------       |   ------  |  ------  |\n",
      "| Model-2 | Engineered Features Data |   0.7189  | 75.85 %  |\n",
      "|  ------ |       -----------        |   ------  |  ------  |\n",
      "| Model-3 | Engineered Features Data |   0.5952  | 82.15 %  |\n",
      "+---------+--------------------------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"\\t \\tDEEP LEARNING Architectures \")\n",
    "print(\"\")\n",
    "# Architecture of layers\n",
    "arch1=\"1-Layers LSTM\"\"\\n\"\" LSTM1(100) - Dense(output) ->> Rmsprop+Dropout(0.4)\"\n",
    "arch2=\"1-Layer LSTM:\"\"\\n\"\" LSTM1(40) - Dense(output) ->> Adam+Dropout(0.2)\"\n",
    "arch3=\"2-Layer MLP:\"\"\\n\"\" Dense(500) - Dense(200) - Dense(output) ->> Adam+Dropout(0.3)+BN()\"\n",
    "\n",
    "ls=[arch1,arch2,arch3]\n",
    "for i,l in enumerate(ls,1):\n",
    "    print(\" MODEL-{0} Architecture: {1}\\n\".format(i,l))\n",
    "\n",
    "# pretty table\n",
    "from prettytable import PrettyTable\n",
    "T = PrettyTable()\n",
    "T.field_names = [\"Models\",\"Data\",\"Test-Loss\", \"Accuracy\"]\n",
    "T.add_row([\"Model-1\",\"Raw Signal Data\",\"0.9572\", \"68.33 %\"])\n",
    "T.add_row([\"------\",\"------------\",\"------\",\"------\"])\n",
    "T.add_row([\"Model-2\",\"Engineered Features Data\",\"0.7189\", \"75.85 %\"])\n",
    "T.add_row([\"------\",\"-----------\",\"------\",\"------\"])\n",
    "T.add_row([\"Model-3\",\"Engineered Features Data\",\"0.5952\", \"82.15 %\"])\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Steps Followed to solve this problem</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1.   **Problem Definition:** This include clearly understanding the problem being solved\n",
    "  *  In this Project, you’ll help robots **recognize the floor surface** they’re standing on using data collected from Inertial Measurement Units (IMU sensors).\n",
    "  *   This project is to build a model that **predicts the Surface** such as Soft tiles,Hard tiled, Wood ect. on which the robot navigates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2.  **Gathering Data:** The quality and quantity of data that you gather will determine how good your predictive model can be.\n",
    "  * Collected **IMU sensor** data while driving a small mobile robot over different floor **surfaces** on the university premise and you'll help improve the navigation of robots without assistance across many different surfaces, so they won’t fall down on the job.\n",
    "  * dataset from **Kaggle:** https://www.kaggle.com/c/career-con-2019/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. **Data Analysis and Preprocessing :** This includes gaining insights from data to solve a problem and cleaning unwanted data\n",
    "    * Data collected from IMU Sensor contain has 128 readings for each data points with 13 Features for every reading.\n",
    "    * **Preprocessing Include:**\n",
    "      * Checking **Nan/Null** Values.\n",
    "      * Checking for **Duplicates** in Data.\n",
    "      * **Rearranging** data to a required shape and format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. **Data preparation:** Discovering the format of data that the machine learning model can understand and construction of  features like:\n",
    " * This includes extracting data like coming up with **Statistical Features** for each Datapoint. \n",
    " * **example:** Mean, median, std ect for each signal Feature.\n",
    " * Obtaining **Fourier Features** from data for modeling ect.\n",
    " * **Encoding** target variables to **Numerics** for ML Models and **One-Hot-Encoding** for Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5. **Choosing a Model and Methods:** There are many models that researchers have created over the years but in this case study we will experiment with a bunch of  algorithms \n",
    " * we use **Machine Learning** model below :\n",
    "      *  Logistic Regression \n",
    "      *  Linear SVC         \n",
    "      *  RBF SVM classifier \n",
    "      *  DecisionTree    \n",
    "      * Random Forest    \n",
    "      * GradientBoosting DT\n",
    "      * XG-Boost\n",
    " * we also use **Deep Learning** Model below :\n",
    "      * LSTM\n",
    "      * MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6. **Training:** The is considered the bulk of machine learning  which refer to Building a Machine Learning model\n",
    " * In this case study we make a **70-30 Split** for Training, Testing and for building a model.\n",
    " * Use **Engineered  features** for machine learning models but we use both **Raw Data** 128-vector and **Engineered Features** for the deep learnig models respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 7. **Evaluation:**  This is where we evaluate model performance how good is our model\n",
    "  * For deep learning model goal is to reduce **Multi-class log loss**\n",
    "  * Use **Accuracy** as main Mertic to measure performance of Model\n",
    "  * Also use **Confusion Matrix** as mertic of Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 8.  **Improve Results(Parameter Tuning):**  This is where Hyper parameter tuning is done to gain best model possible\n",
    "  * **GridSearchCV** for Hyper Parameter Tunning on all models.\n",
    "  * Try a bunch of **Hyper Parameter values** to increase  Accuracy  more as possible.\n",
    "  * Use **Learning_rate_reduction** and **ModelCheckpoint** method to enhance results.\n",
    "  * To improve performance we can come up with more **Domain Specific features** that can improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 9. **Present results:** The results of model performance for this case study is Represented in **pretty table** above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION:**\n",
    "* In the real world, domain-knowledge, EDA and feature-engineering matter most.\n",
    "* The machine learning model perform better than Deep Learning Models.\n",
    "* There is still more amount of misclassication which has to be improved.\n",
    "* The Imbalance data set is responsible for low Accuracy For all models.<br>\n",
    "Finally our best machine learning model is GradientBoosting DT and deep learning model is MLP with model-3 Architecture which out perform every other models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
